{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import KFold\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_encoding(path, outfile):\n",
    "    dataMat = []; labelMat = []\n",
    "    dataList = []; labelList = []\n",
    "    feature_val = []\n",
    "    encoded_20D = []\n",
    "    result_1D = []\n",
    "    \n",
    "    files = os.listdir(path)\n",
    "    for file in files:\n",
    "        if not os.path.isdir(file):  #open when it is a folder\n",
    "            f = open(path+'/'+file)\n",
    "            lines = f.readlines()  # return a list\n",
    "            for line in lines:\n",
    "                #separate the sequences and labels\n",
    "                l_line = line.strip()\n",
    "                line_list = l_line.split(' ')\n",
    "                while '' in line_list:\n",
    "                    line_list.remove('')\n",
    "\n",
    "                dataList.append(line_list[0])\n",
    "                labelList.append(line_list[1])\n",
    "    '''\n",
    "    for seq in dataList:\n",
    "        for i in range(len(seq)):\n",
    "            # Encode each amino acid in the sequence into a sequence of 20 as 0，1 binaries\n",
    "            binary_str_20D = dic_encoded[seq[i]]\n",
    "            for j in range(len(binary_str_20D)):\n",
    "                temp_v = int(binary_str_20D[j])\n",
    "                feature_val.append(temp_v)\n",
    "    step = 180\n",
    "    encoded_20D = [feature_val[i:i+step] for i in range(0,len(feature_val),step)]\n",
    "\n",
    "    '''\n",
    "    \n",
    "    for i in range(len(labelList)):\n",
    "        result_1D.append(int(labelList[i]))\n",
    "    # dataMat = np.array(encoded_20D) #dataMat as a X feature matrix\n",
    "    \n",
    "    dataMat = np.array(dataList) #dataMat as a X feature matrix\n",
    "    labelMat = np.asarray(result_1D) #labelMat as a y label matrix\n",
    "    # Combine the feature matrix and the label matrix into one matrix\n",
    "    window_Mat = np.column_stack((dataMat, labelMat))\n",
    "    np.random.shuffle(window_Mat)\n",
    "    print(window_Mat.shape)\n",
    "\n",
    "    np.savetxt(outfile,window_Mat,fmt='%s')\n",
    "    f.close()\n",
    "    return window_Mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataSetbyKFold(window_Mat,split_size,outdir):\n",
    "    if not os.path.exists(outdir): #if not outdir, makedir\n",
    "        os.makedirs(outdir)\n",
    "    train_all = [];\n",
    "    test_all = []\n",
    "    each_split_tr = []\n",
    "    each_split_te = []\n",
    "    count_split = 0\n",
    "    kf = KFold(n_splits=split_size)\n",
    "    for train_index, test_index in kf.split(window_Mat):\n",
    "        count_split += 1\n",
    "        for index in train_index:\n",
    "            each_split_tr.append(list(window_Mat[index]))\n",
    "        array_ = np.array(each_split_tr)\n",
    "        np.savetxt(outdir + \"/train_\" + str(count_split) + '.txt',array_, fmt=\"%s\", delimiter='\\t')  # output each piece of data\n",
    "        train_all.append(each_split_tr)  # Add each piece of data to a list '[[[],[],...[]]]' 3-D list\n",
    "        each_split_tr = []\n",
    "\n",
    "        for index in test_index:\n",
    "            each_split_te.append(list(window_Mat[index]))\n",
    "        array_ = np.array(each_split_te)\n",
    "        np.savetxt(outdir + \"/test_\" + str(count_split) + '.txt',array_, fmt=\"%s\", delimiter='\\t')  # output each piece of data\n",
    "        test_all.append(each_split_te)  # Add each piece of data to a list\n",
    "        each_split_te = []\n",
    "\n",
    "    #train_all = train_all[0]\n",
    "    #test_all = test_all[0]\n",
    "    return train_all, test_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(labelArr, predictArr):#类标签为int类型\n",
    "    #labelArr[i] is actual value,predictArr[i] is predict value\n",
    "    TP = 0.0; TN = 0.0; FP = 0.0; FN = 0.0\n",
    "    for i in range(len(labelArr)):\n",
    "        if labelArr[i] == 1 and predictArr[i] == 1:\n",
    "            TP += 1.0\n",
    "        if labelArr[i] == 1 and predictArr[i] == 0:\n",
    "            FN += 1.0\n",
    "        if labelArr[i] == 0 and predictArr[i] == 1:\n",
    "            FP += 1.0\n",
    "        if labelArr[i] == 0 and predictArr[i] == 0:\n",
    "            TN += 1.0\n",
    "    print(TP)\n",
    "    print(FN)\n",
    "    print(TN)\n",
    "    print(FP)\n",
    "    \n",
    "    if ((TP+FN) == 0):\n",
    "        SN = 0\n",
    "        SP = 0\n",
    "    elif ((FP+FN) == 0):\n",
    "        SN = 0\n",
    "        SP = 0\n",
    "    else:\n",
    "        SN = TP/(TP + FN) #Sensitivity = TP/P  and P = TP + FN\n",
    "        SP = TN/(FP + TN) #Specificity = TN/N  and N = TN + FP\n",
    "\n",
    "    #MCC = (TP*TN-FP*FN)/math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))\n",
    "    return SN,SP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(clf,clfname,train_X, train_y, test_X, test_y,i):#X:feature matrix，y:label matrix\n",
    "    # train with train set\n",
    "    print(\" training begin...\")\n",
    "    clf = clf.fit(train_X,train_y)\n",
    "    print(\" training end.\")\n",
    "    #==========================================================================\n",
    "    # test with validation set\n",
    "    print(\" test begin.\")\n",
    "    predict_ = clf.predict(test_X) #return type is float64\n",
    "    proba = clf.predict_proba(test_X)[:,1] #return type is float64\n",
    "    score_ = clf.score(test_X, test_y)\n",
    "    \n",
    "    # Report\n",
    "    sk_report = classification_report(\n",
    "    digits=6,\n",
    "    y_true=test_y, \n",
    "    y_pred=clf.predict(test_X))\n",
    "    print(sk_report)\n",
    "    \n",
    "    print(\" test end.\")\n",
    "    \n",
    "    #==========================================================================\n",
    "    \n",
    "    ACC = accuracy_score(test_y, predict_)\n",
    "    SN, SP = performance(test_y, predict_)\n",
    "    MCC = matthews_corrcoef(test_y, predict_)\n",
    "    #AUC = roc_auc_score(test_y, proba)\n",
    "    AUC = 0\n",
    "    \n",
    "    # Model Evaluation\n",
    "    #==========================================================================\n",
    "    #save output\n",
    "    \n",
    "    eval_output = []\n",
    "    eval_output.append(ACC);eval_output.append(SN);eval_output.append(AUC)\n",
    "    eval_output.append(SP);eval_output.append(MCC)\n",
    "    eval_output.append(score_)\n",
    "    eval_output = np.array(eval_output,dtype=float)\n",
    "    \n",
    "    np.savetxt(\"proba.data\",proba,fmt=\"%f\",delimiter=\"\\t\")\n",
    "    np.savetxt(\"test_y.data\",test_y,fmt=\"%f\",delimiter=\"\\t\")\n",
    "    np.savetxt(\"predict.data\",predict_,fmt=\"%f\",delimiter=\"\\t\")\n",
    "    #np.savetxt(\"eval_output.data\",eval_output,fmt=\"%f\",delimiter=\"\\t\")\n",
    "    print(\"Wrote results to output.data...EOF...\")\n",
    "    # ==========================================================================\n",
    "    # save Model\n",
    "    os.chdir(\"/Users/oscarkuan/coding/fyp_ecgid/ML_Model\")\n",
    "\n",
    "    joblib.dump(clf,'train_'+clfname+str(i)+'.model')\n",
    "    return ACC,SN,SP,MCC,AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_fun used to find the average value of the values in the list,\n",
    "# mainly ACC mean,SP mean and SN mean, to evaluate the model\n",
    "def mean_fun(onelist):\n",
    "    count = 0\n",
    "    for i in onelist:\n",
    "        count += i\n",
    "    return float(count/len(onelist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossValidation(clf, clfname, curdir, train_all, test_all):\n",
    "    os.chdir(curdir)\n",
    "    cur_path = curdir\n",
    "    ACCs = [];SNs = [];SPs = [];MCCs = [];AUCs = []\n",
    "\n",
    "    for i in range(len(train_all)):\n",
    "        os.chdir(cur_path)\n",
    "        train_data = train_all[i]; train_X = []; train_y = []\n",
    "        test_data = test_all[i]; test_X = []; test_y = []\n",
    "\n",
    "        #Divide train_all into train_X and train_y\n",
    "        for eachline_train in train_data:\n",
    "            one_train = eachline_train\n",
    "            one_train_format = []\n",
    "            for index in range(0, len(one_train) - 1):\n",
    "                one_train_format.append(float(one_train[index]))\n",
    "            train_X.append(one_train_format)\n",
    "            train_y.append(int(one_train[-1]))\n",
    "\n",
    "        #Divide test_all into test_X and test_y\n",
    "        for eachline_test in test_data:\n",
    "            one_test = eachline_test\n",
    "            one_test_format = []\n",
    "            for index in range(0, len(one_test) - 1):\n",
    "                one_test_format.append(float(one_test[index]))\n",
    "            test_X.append(one_test_format)\n",
    "            test_y.append(int(one_test[-1]))\n",
    "        # ======================================================================\n",
    "        # classifier start here\n",
    "        if not os.path.exists(clfname):\n",
    "            os.mkdir(clfname)\n",
    "        out_path = clfname + \"/\" + clfname + \"_00\" + str(i)  # the folder that save result of each fold\n",
    "        if not os.path.exists(out_path):\n",
    "            os.mkdir(out_path)\n",
    "        os.chdir(out_path)\n",
    "        ACC, SN, SP, MCC, AUC = classifier(clf, clfname, train_X, train_y, test_X, test_y,i)\n",
    "        ACCs.append(ACC);\n",
    "        SNs.append(SN);\n",
    "        SPs.append(SP);\n",
    "        MCCs.append(MCC);\n",
    "        AUCs.append(AUC)\n",
    "    # ======================================================================\n",
    "    ACC_mean = mean_fun(ACCs)\n",
    "    SN_mean = mean_fun(SNs)\n",
    "    SP_mean = mean_fun(SPs)\n",
    "    MCC_mean = mean_fun(MCCs)\n",
    "    AUC_mean = mean_fun(AUCs)\n",
    "    # ==========================================================================\n",
    "    # output experiment result\n",
    "    (\"/Users/oscarkuan/coding/fyp_ecgid/\")\n",
    "    os.system(\"echo `date`'\" + str(clf) + \"' >> log.out\")\n",
    "    os.system(\"echo ACC_mean=\" + str(ACC_mean) + \" >> log.out\")\n",
    "    os.system(\"echo SN_mean=\" + str(SN_mean) + \" >> log.out\")\n",
    "    os.system(\"echo SP_mean=\" + str(SP_mean) + \" >> log.out\")\n",
    "    os.system(\"echo MCC_mean=\" + str(MCC_mean) + \" >> log.out\")\n",
    "    os.system(\"echo AUC_mean=\" + str(AUC_mean) + \" >> log.out\")\n",
    "    return ACC_mean, SN_mean, SP_mean, MCC_mean, AUC_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 2)\n",
      "generate Dataset end and cross validation start\n",
      " training begin...\n",
      " training end.\n",
      " test begin.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1   0.333333  0.700000  0.451613        50\n",
      "           2   0.315789  0.279070  0.296296        43\n",
      "           3   0.700000  0.600000  0.646154        35\n",
      "           4   0.280702  0.533333  0.367816        30\n",
      "           5   0.681818  0.652174  0.666667        46\n",
      "           6   0.234043  0.275000  0.252874        40\n",
      "           7   0.433333  0.317073  0.366197        41\n",
      "           8   0.900000  0.818182  0.857143        33\n",
      "           9   0.631579  0.279070  0.387097        43\n",
      "          10   0.000000  0.000000  0.000000        39\n",
      "\n",
      "    accuracy                       0.442500       400\n",
      "   macro avg   0.451060  0.445390  0.429186       400\n",
      "weighted avg   0.446291  0.442500  0.424245       400\n",
      "\n",
      " test end.\n",
      "35.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "Wrote results to output.data...EOF...\n",
      " training begin...\n",
      " training end.\n",
      " test begin.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1   0.297872  0.736842  0.424242        38\n",
      "           2   0.257143  0.243243  0.250000        37\n",
      "           3   0.588235  0.526316  0.555556        38\n",
      "           4   0.307692  0.540541  0.392157        37\n",
      "           5   0.652174  0.652174  0.652174        46\n",
      "           6   0.203704  0.289474  0.239130        38\n",
      "           7   0.260870  0.127660  0.171429        47\n",
      "           8   0.846154  0.814815  0.830189        27\n",
      "           9   0.304348  0.145833  0.197183        48\n",
      "          10   0.000000  0.000000  0.000000        44\n",
      "\n",
      "    accuracy                       0.382500       400\n",
      "   macro avg   0.371819  0.407690  0.371206       400\n",
      "weighted avg   0.355069  0.382500  0.350040       400\n",
      "\n",
      " test end.\n",
      "28.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "Wrote results to output.data...EOF...\n",
      " training begin...\n",
      " training end.\n",
      " test begin.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1   0.265957  0.531915  0.354610        47\n",
      "           2   0.155556  0.162791  0.159091        43\n",
      "           3   0.535714  0.348837  0.422535        43\n",
      "           4   0.240741  0.325000  0.276596        40\n",
      "           5   0.666667  0.702703  0.684211        37\n",
      "           6   0.176471  0.230769  0.200000        39\n",
      "           7   0.363636  0.216216  0.271186        37\n",
      "           8   0.897436  0.875000  0.886076        40\n",
      "           9   0.214286  0.181818  0.196721        33\n",
      "          10   0.000000  0.000000  0.000000        41\n",
      "\n",
      "    accuracy                       0.360000       400\n",
      "   macro avg   0.351646  0.357505  0.345103       400\n",
      "weighted avg   0.349567  0.360000  0.344562       400\n",
      "\n",
      " test end.\n",
      "25.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "Wrote results to output.data...EOF...\n",
      " training begin...\n",
      " training end.\n",
      " test begin.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1   0.337209  0.580000  0.426471        50\n",
      "           2   0.250000  0.257143  0.253521        35\n",
      "           3   0.656250  0.617647  0.636364        34\n",
      "           4   0.368421  0.717949  0.486957        39\n",
      "           5   0.564103  0.758621  0.647059        29\n",
      "           6   0.173077  0.209302  0.189474        43\n",
      "           7   0.176471  0.085714  0.115385        35\n",
      "           8   0.954545  0.840000  0.893617        50\n",
      "           9   0.444444  0.228571  0.301887        35\n",
      "          10   0.000000  0.000000  0.000000        50\n",
      "\n",
      "    accuracy                       0.427500       400\n",
      "   macro avg   0.392452  0.429495  0.395073       400\n",
      "weighted avg   0.388880  0.427500  0.392555       400\n",
      "\n",
      " test end.\n",
      "29.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "Wrote results to output.data...EOF...\n",
      " training begin...\n",
      " training end.\n",
      " test begin.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1   0.222222  0.666667  0.333333        33\n",
      "           2   0.205882  0.205882  0.205882        34\n",
      "           3   0.633333  0.500000  0.558824        38\n",
      "           4   0.288136  0.566667  0.382022        30\n",
      "           5   0.530612  0.666667  0.590909        39\n",
      "           6   0.269231  0.333333  0.297872        42\n",
      "           7   0.285714  0.195122  0.231884        41\n",
      "           8   0.838710  0.650000  0.732394        40\n",
      "           9   0.277778  0.098039  0.144928        51\n",
      "          10   0.000000  0.000000  0.000000        52\n",
      "\n",
      "    accuracy                       0.360000       400\n",
      "   macro avg   0.355162  0.388238  0.347805       400\n",
      "weighted avg   0.346187  0.360000  0.331116       400\n",
      "\n",
      " test end.\n",
      "22.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "Wrote results to output.data...EOF...\n",
      " training begin...\n",
      " training end.\n",
      " test begin.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1   0.344828  0.600000  0.437956        50\n",
      "           2   0.184211  0.194444  0.189189        36\n",
      "           3   0.666667  0.465116  0.547945        43\n",
      "           4   0.371795  0.557692  0.446154        52\n",
      "           5   0.666667  0.774194  0.716418        31\n",
      "           6   0.203704  0.289474  0.239130        38\n",
      "           7   0.354839  0.250000  0.293333        44\n",
      "           8   1.000000  0.750000  0.857143        32\n",
      "           9   0.227273  0.147059  0.178571        34\n",
      "          10   0.000000  0.000000  0.000000        40\n",
      "\n",
      "    accuracy                       0.402500       400\n",
      "   macro avg   0.401998  0.402798  0.390584       400\n",
      "weighted avg   0.389051  0.402500  0.382932       400\n",
      "\n",
      " test end.\n",
      "30.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "Wrote results to output.data...EOF...\n",
      " training begin...\n",
      " training end.\n",
      " test begin.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1   0.262626  0.577778  0.361111        45\n",
      "           2   0.348837  0.326087  0.337079        46\n",
      "           3   0.480000  0.400000  0.436364        30\n",
      "           4   0.322034  0.452381  0.376238        42\n",
      "           5   0.611111  0.488889  0.543210        45\n",
      "           6   0.295455  0.333333  0.313253        39\n",
      "           7   0.111111  0.055556  0.074074        36\n",
      "           8   0.795455  0.813953  0.804598        43\n",
      "           9   0.156250  0.111111  0.129870        45\n",
      "          10   0.000000  0.000000  0.000000        29\n",
      "\n",
      "    accuracy                       0.372500       400\n",
      "   macro avg   0.338288  0.355909  0.337580       400\n",
      "weighted avg   0.350122  0.372500  0.351046       400\n",
      "\n",
      " test end.\n",
      "26.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "Wrote results to output.data...EOF...\n",
      " training begin...\n",
      " training end.\n",
      " test begin.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1   0.323232  0.627451  0.426667        51\n",
      "           2   0.250000  0.210526  0.228571        38\n",
      "           3   0.717949  0.560000  0.629213        50\n",
      "           4   0.369231  0.533333  0.436364        45\n",
      "           5   0.604167  0.783784  0.682353        37\n",
      "           6   0.279070  0.315789  0.296296        38\n",
      "           7   0.157895  0.093750  0.117647        32\n",
      "           8   0.933333  0.756757  0.835821        37\n",
      "           9   0.280000  0.269231  0.274510        26\n",
      "          10   0.000000  0.000000  0.000000        46\n",
      "\n",
      "    accuracy                       0.427500       400\n",
      "   macro avg   0.391488  0.415062  0.392744       400\n",
      "weighted avg   0.395806  0.427500  0.399691       400\n",
      "\n",
      " test end.\n",
      "32.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "Wrote results to output.data...EOF...\n",
      " training begin...\n",
      " training end.\n",
      " test begin.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1   0.250000  0.488889  0.330827        45\n",
      "           2   0.230769  0.257143  0.243243        35\n",
      "           3   0.600000  0.545455  0.571429        33\n",
      "           4   0.397260  0.604167  0.479339        48\n",
      "           5   0.609756  0.675676  0.641026        37\n",
      "           6   0.333333  0.377778  0.354167        45\n",
      "           7   0.304348  0.194444  0.237288        36\n",
      "           8   0.882353  0.789474  0.833333        38\n",
      "           9   0.476190  0.238095  0.317460        42\n",
      "          10   0.000000  0.000000  0.000000        41\n",
      "\n",
      "    accuracy                       0.417500       400\n",
      "   macro avg   0.408401  0.417112  0.400811       400\n",
      "weighted avg   0.400606  0.417500  0.396160       400\n",
      "\n",
      " test end.\n",
      "22.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "Wrote results to output.data...EOF...\n",
      " training begin...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    path = 'LABELED_DATASET'\n",
    "    outfile = 'seq_encoded.txt'\n",
    "    outdir = 'KFold'\n",
    "    a = []\n",
    "    # encode the original dataset\n",
    "    window_Mat = seq_encoding(path,outfile)\n",
    "\n",
    "    # split the feature matrix into N fold\n",
    "    train_all, test_all = splitDataSetbyKFold(window_Mat, 100, outdir)\n",
    "\n",
    "    print(\"generate Dataset end and cross validation start\")\n",
    "\n",
    "    clf = svm.SVC(C=1, kernel='rbf', gamma=0.05, probability=True)\n",
    "    curdir = '/Users/oscarkuan/coding/fyp_ecgid/'\n",
    "    clfname = 'SVM'\n",
    "    #ACC_mean, SN_mean, SP_mean, MCC_mean, AUC_mean = crossValidation(clf, clfname, curdir, train_all, test_all)\n",
    "    crossValidation(clf, clfname, curdir, train_all, test_all)\n",
    "\n",
    "    # performace_list = [ACC_mean, SN_mean, SP_mean, MCC_mean, AUC_mean]\n",
    "    # performace_set = ['ACC_mean', 'SN_mean', 'SP_mean', 'MCC_mean', 'AUC_mean']\n",
    "    # plt.plot(performace_set, performace_list, 'r-o', label='Performance')\n",
    "    # plt.legend()\n",
    "    # plt.title('MHC Prediction by SVM')\n",
    "    # plt.xlabel('Name of Evaluation')\n",
    "    # plt.ylabel('Performance')\n",
    "    # plt.show()\n",
    "    # plt.savefig('SVM_10fold.png')\n",
    "    #print('MCC_mean', '\\t', 'AUC_mean', '\\t', 'ACC_mean', '\\t', 'SN_mean', '\\t', 'SP_mean')\n",
    "    #print(MCC_mean, AUC_mean, ACC_mean, SN_mean, SP_mean)  # 将ACC均值，SP均值，SN均值都输出到控制台\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
